{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder is an unsupervised artificial neural network that learns how to efficiently compress and encode data then learns how to reconstruct the data back from the reduced encoded representation to a representation that is as close to the original input as possible.\n",
    "\n",
    "Autoencoder, by design, reduces data dimensions by learning how to ignore the noise in the data.\n",
    "\n",
    "![title](autoencoder.png)\n",
    "\n",
    "Autoencoder Components:\n",
    "\n",
    "Autoencoders consists of 4 main parts:\n",
    "\n",
    "1. **Encoder**: In which the model learns how to reduce the input dimensions and compress the input data into an encoded representation.\n",
    "\n",
    "2. **Bottleneck**: which is the layer that contains the compressed representation of the input data. This is the lowest possible dimensions of the input data.\n",
    "\n",
    "3. **Decoder**: In which the model learns how to reconstruct the data from the encoded representation to be as close to the original input as possible.\n",
    "\n",
    "4. **Reconstruction Loss**: This is the method that measures measure how well the decoder is performing and how close the output is to the original input.\n",
    "\n",
    "The training then involves using back propagation in order to minimize the networkâ€™s reconstruction loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#1FC17B', '#78FECF', '#555B6E', '#CC998D', '#429EA6',\n",
    "          '#153B50', '#8367C7', '#EE6352', '#C287E8', '#F0A6CA', \n",
    "          '#521945', '#361F27', '#828489', '#9AD2CB', '#EBD494', \n",
    "          '#53599A', '#80DED9', '#EF2D56', '#446DF6', '#AF929D']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use sklearn make_blobs function to create a toy dataset. <code>make_blobs</code> function will create a dataset based on the parameter passed by the user. Here X values are the location of the data and y being the cluster label for the data.\n",
    "<br>\n",
    "<br>\n",
    "**n_features** is the number of features we are expecting\n",
    "<br>\n",
    "**centers** tells it to create as many centers of data, meaning how many clusters of data should be in the dataset\n",
    "<br>\n",
    "**n_samples** requests the number of datapoints in the dataset\n",
    "<br>\n",
    "**cluster_std** tells it to limit the standard deviation of every cluster to 0.2\n",
    "<br>\n",
    "**center_box** limits the upper and lower bounds for the center states in the clusters\n",
    "<br>\n",
    "**random_state** is for reproduceability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_features = 50, centers = 20, n_samples = 20000,\n",
    "                 cluster_std = 0.2, center_box = [-1, 1], random_state = 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the data\n",
    "# You see that there are 100 data points generated and their standard deviation is \n",
    "print(X[0])\n",
    "\n",
    "print(f'Standard deviation among clusters is: {X[0].std():.3f}')\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting our data in train and test folds\n",
    "# Scaling the dataset using the MinMaxScaler so that each data point will be scaled between 0 and 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a baseline model using PCA\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "pca = pca.fit(X_train)\n",
    "\n",
    "# Fit the model on X_test (transform the X_test data according the preprocessing on the training data)\n",
    "results_pca = pca.transform(X_test)\n",
    "results_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results of the PCA on a scatter plot to determine how the PCA has performed\n",
    "\n",
    "print(results_pca.shape)\n",
    "\n",
    "unique_label = np.unique(y_test)\n",
    "\n",
    "# Plotting the scatter by each cluster\n",
    "\n",
    "for index, label in enumerate(unique_label):\n",
    "    X_data = results_pca[y_test == unique_label[index]]\n",
    "    \n",
    "    # X_data[:, 0] is the first PCA dimension (x axis)\n",
    "    # X_data[:, 1] is the second PCA dimension (y axis)\n",
    "    # alpha is to give each point a bit of transperancy so that if points are plotted over each other we can still see those\n",
    "    # One color for each cluster\n",
    "    \n",
    "    plt.scatter(X_data[:, 0], X_data[:, 1], alpha = 0.3, c = colors[index])\n",
    "\n",
    "plt.xlabel('Principle Component #1')\n",
    "plt.ylabel('Principle Component #2')\n",
    "plt.title('PCA Results')\n",
    "\n",
    "# Looking at the below plot, it seems that there are 10 clear, well-separated clusters instead of 20. This could be\n",
    "# an issue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are going to build a multilayer perceptron (where each node in the hidden layer is a weighted, non-linear\n",
    "# combination of all the other nodes in the previous layer).\n",
    "\n",
    "# alpha = Learning rate is a hyperparameter that determines how much the model moves in the direction of reducing \n",
    "# the loss function\n",
    "\n",
    "# Defining the neural network achitecture\n",
    "\n",
    "autoencoder = MLPRegressor(alpha = 1.0e-15,\n",
    "                          hidden_layer_sizes = (50, 100, 50, 2, 50, 100, 50),\n",
    "                          random_state = 17,\n",
    "                          max_iter = 10000)\n",
    "\n",
    "autoencoder.fit(X_train, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have built the autoencoder, we are going to pull some of its features. Here the coefficients are the \n",
    "# weights and the intercepts are like constant in y = mx + c equation.\n",
    "\n",
    "# Since the autoencoder builds an encoder and a decoder, for this task we only need the encoder part (which is the \n",
    "# first four steps of the model) so we will pull it out of the model\n",
    "\n",
    "W = autoencoder.coefs_\n",
    "biases = autoencoder.intercepts_\n",
    "\n",
    "for w in W:\n",
    "    print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_weights = W[0:4]\n",
    "encoder_biases = biases[0:4]\n",
    "print(encoder_weights)\n",
    "print(encoder_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(data, encoder_weights, encoder_biases):\n",
    "    results_ae = data\n",
    "    \n",
    "    for index, (W, b) in enumerate(zip(encoder_weights, encoder_biases)):\n",
    "        if results_ae.any() == len(encoder_weights):\n",
    "            results_ae = results_ae@W + b\n",
    "        else:\n",
    "            results_ae = np.maximum(0, results_ae@W + b) # Creates a dot product function by giving the market @\n",
    "    \n",
    "    return results_ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ae = encode(X_test, encoder_weights, encoder_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_ae.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, label in enumerate(unique_label):\n",
    "    latent_space = res_ae[y_test == unique_label[index]]\n",
    "    \n",
    "    plt.scatter(latent_space[:, 0], latent_space[:, 1], alpha = 0.3, c = colors[index])\n",
    "\n",
    "plt.xlabel('Latent X')\n",
    "plt.ylabel('Latent Y')\n",
    "plt.title('Encoder Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretation of Silhouette Score - it ranges from -1 to +1, where -1 is the very poor clustering and +1 is the\n",
    "# perfect clustering\n",
    "# Here it's shown that the encoder is working better than the PCA since the encoder has a higher silhouette score\n",
    "\n",
    "print(silhouette_score(X_test, y_test)) # Original data using all 50 dimensions\n",
    "print(silhouette_score(results_pca, y_test)) # PCA\n",
    "print(silhouette_score(res_ae, y_test)) # autoencoder\n",
    "\n",
    "# The autoencoder seems to have done better that the original data itself to classifiy each of the labels in the \n",
    "# cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders are really cool and powerful. You can encode data into lower dimensional space, like we did, and find better clusters than we initially created, which is quite an amazing result! You can also use the decoder to create new datasets in a generative approach by sampling inside your latent space.\n",
    "\n",
    "Autoencoders can be used as compression algorithms, where the decoder recovers a compressed file, like a photo. They can also be used to denoise data -- by adding noise to input, and learning to recreate the noise-free data, you can achieve this.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
